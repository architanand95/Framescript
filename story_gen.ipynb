{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n!pip install einops","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import warnings\nfrom typing import Any, Dict, Tuple\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"A new version of the following files was downloaded from\")","metadata":{"execution":{"iopub.status.busy":"2024-04-16T15:29:21.638227Z","iopub.execute_input":"2024-04-16T15:29:21.639025Z","iopub.status.idle":"2024-04-16T15:29:31.414265Z","shell.execute_reply.started":"2024-04-16T15:29:21.638995Z","shell.execute_reply":"2024-04-16T15:29:31.413257Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Constants for prompt generation\nINSTRUCTION_KEY = \"### Instruction:\"\nRESPONSE_KEY = \"### Response:\"\nEND_KEY = \"### End\"\nINTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\nPROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n{instruction_key}\n{instruction}\n{response_key}\n\"\"\".format(\n    intro=INTRO_BLURB,\n    instruction_key=INSTRUCTION_KEY,\n    instruction=\"{instruction}\",\n    response_key=RESPONSE_KEY,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T15:29:31.415927Z","iopub.execute_input":"2024-04-16T15:29:31.416320Z","iopub.status.idle":"2024-04-16T15:29:31.421249Z","shell.execute_reply.started":"2024-04-16T15:29:31.416296Z","shell.execute_reply":"2024-04-16T15:29:31.420463Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class InstructionTextGenerationPipeline:\n    def __init__(\n        self,\n        model_name,\n        torch_dtype=torch.bfloat16,\n        trust_remote_code=True,\n        use_auth_token=None,\n    ) -> None:\n        # Load model\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch_dtype,\n            trust_remote_code=trust_remote_code,\n            use_auth_token=use_auth_token,\n        )\n\n        # Load the tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name,\n            trust_remote_code=trust_remote_code,\n            use_auth_token=use_auth_token,\n        )\n        if tokenizer.pad_token_id is None:\n            warnings.warn(\n                \"pad_token_id is not set for the tokenizer. Using eos_token_id as pad_token_id.\"\n            )\n            tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.padding_side = \"left\"\n        self.tokenizer = tokenizer\n        # Device to use (GPU or CPU)\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model.eval()\n        self.model.to(device=device, dtype=torch_dtype)\n\n        # Set generation parameters\n        self.generate_kwargs = {\n            \"temperature\": 0.1,\n            \"top_p\": 0.92,\n            \"top_k\": 0,\n            \"max_new_tokens\": 1024,\n            \"use_cache\": True,\n            \"do_sample\": True,\n            \"eos_token_id\": self.tokenizer.eos_token_id,\n            \"pad_token_id\": self.tokenizer.pad_token_id,\n            \"repetition_penalty\": 1.1,  # 1.0 means no penalty, > 1.0 means penalty, 1.2 from CTRL paper\n        }\n\n    def format_instruction(self, instruction):\n        return PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction)\n\n    def __call__(\n        self, instruction: str, **generate_kwargs: Dict[str, Any]\n    ) -> Tuple[str, str, float]:\n        # Format the prompt with the provided instruction\n        s = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction)\n        \n        # Tokenize the prompt and convert to tensors\n        input_ids = self.tokenizer(s, return_tensors=\"pt\").input_ids\n        input_ids = input_ids.to(self.model.device)\n        \n        # Set the generation parameters\n        gkw = {**self.generate_kwargs, **generate_kwargs}\n        \n        with torch.no_grad():\n            # Generate the output sequence\n            output_ids = self.model.generate(input_ids, **gkw)\n        \n        # Slice the output_ids tensor to get only new tokens\n        new_tokens = output_ids[0, len(input_ids[0]) :]\n        \n        # Decode the new tokens into text\n        output_text = self.tokenizer.decode(new_tokens, skip_special_tokens=True)\n        \n        return output_text","metadata":{"execution":{"iopub.status.busy":"2024-04-16T15:29:31.422714Z","iopub.execute_input":"2024-04-16T15:29:31.423097Z","iopub.status.idle":"2024-04-16T15:29:31.436889Z","shell.execute_reply.started":"2024-04-16T15:29:31.423064Z","shell.execute_reply":"2024-04-16T15:29:31.436232Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"pip install einops","metadata":{"execution":{"iopub.status.busy":"2024-04-16T15:30:42.558522Z","iopub.execute_input":"2024-04-16T15:30:42.559247Z","iopub.status.idle":"2024-04-16T15:30:56.618185Z","shell.execute_reply.started":"2024-04-16T15:30:42.559191Z","shell.execute_reply":"2024-04-16T15:30:56.617004Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m621.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.7.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize the model and tokenizer\ngenerate = InstructionTextGenerationPipeline(\n    \"mosaicml/mpt-7b-instruct\",\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n)\nstop_token_ids = generate.tokenizer.convert_tokens_to_ids([\"\"])","metadata":{"execution":{"iopub.status.busy":"2024-04-16T15:31:00.633188Z","iopub.execute_input":"2024-04-16T15:31:00.634084Z","iopub.status.idle":"2024-04-16T15:33:16.410195Z","shell.execute_reply.started":"2024-04-16T15:31:00.634046Z","shell.execute_reply":"2024-04-16T15:33:16.407256Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"norm.py:   0%|          | 0.00/3.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2d88b3f3b454185bab841a574d8e0cf"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n- norm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"flash_attn_triton.py:   0%|          | 0.00/28.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c5cbfc4528e458caf0ffb0cf6b6ded9"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n- flash_attn_triton.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n- norm.py\n- flash_attn_triton.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"blocks.py:   0%|          | 0.00/4.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b63303ded0542faaaa1ccb713c7eed9"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n- blocks.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"warnings.py:   0%|          | 0.00/894 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bc4e5181ea5426fb22c49a4e51974d7"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n- warnings.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n- attention.py\n- blocks.py\n- warnings.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-instruct/7bf8dfd6c819cdb82e2f9d0b251f79ddd33314fb/configuration_mpt.py:114: UserWarning: alibi or rope is turned on, setting `learned_pos_emb` to `False.`\n  warnings.warn(f'alibi or rope is turned on, setting `learned_pos_emb` to `False.`')\n/root/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-instruct/7bf8dfd6c819cdb82e2f9d0b251f79ddd33314fb/configuration_mpt.py:141: UserWarning: If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".\n  warnings.warn(UserWarning('If not using a Prefix Language Model, we recommend setting \"attn_impl\" to \"flash\" instead of \"triton\".'))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_mpt.py:   0%|          | 0.00/32.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14d9c5d2c57f48fea010b9e6f55069fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"custom_embedding.py:   0%|          | 0.00/292 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dc4a4b6bfaa42faad24339ad1916fcf"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n- custom_embedding.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapt_tokenizer.py:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74b6a8b3b28f41ae9b6a6a0d9bd81483"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n- adapt_tokenizer.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"param_init_fns.py:   0%|          | 0.00/11.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f21e832585842fe9f6487b553ba4a65"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n- param_init_fns.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"hf_prefixlm_converter.py:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a05ecddf37b043abaae8e68729abb545"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n- hf_prefixlm_converter.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"meta_init_context.py:   0%|          | 0.00/3.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a191176b352c40d3a5184d40f29e42d0"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n- meta_init_context.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/mosaicml/mpt-7b-instruct:\n- modeling_mpt.py\n- custom_embedding.py\n- adapt_tokenizer.py\n- param_init_fns.py\n- hf_prefixlm_converter.py\n- meta_init_context.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/16.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df092913f09047b29aaa94b44d68bead"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9127680e0c434aa592a0652bab99a672"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9acfe0ee8ac548d482793604c03edb5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.36G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9308f90529f406ba2241fe7cac01d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"102b3b8172af4fedad5edfa0731a2b34"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37d27f044c94434bb3be879d21551cc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a57cb1b95371412aa49964bb6a16573c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d9d22121b27415b8be08e8169cd1035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"062a0d4ed2b74ef796abe04e8bacac97"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/tmp/ipykernel_34/900720683.py:24: UserWarning: pad_token_id is not set for the tokenizer. Using eos_token_id as pad_token_id.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the prompt\narticle=\"\"\"\na brown dog with a green collar is sitting on the grass.\na dog is sniffing something near the car.\na dog is licking the face of another dog\n\n\"\"\"\nprompt = 'Write a three long paragraph story on the three lines of the article' + article\n\n# Generate the text based on the prompt\ngenerated_text = generate(prompt)\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-16T15:37:23.358969Z","iopub.execute_input":"2024-04-16T15:37:23.359650Z","iopub.status.idle":"2024-04-16T15:37:41.720714Z","shell.execute_reply.started":"2024-04-16T15:37:23.359620Z","shell.execute_reply":"2024-04-16T15:37:41.719787Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Here’s one possible version, I hope it works for you!  The first line could be the beginning of a longer narrative about this scene, or it might stand alone as a brief description or summary. \nA brown dog with a green collar sits on the grass in front of a house, looking up at its owner who is standing nearby. The dog seems content and relaxed, and is gently wagging its tail. Near the car parked in the driveway, another dog can be seen sniffing something intently. It appears to be a fellow canine companion, and the two dogs are clearly close friends and enjoy spending time together. The first dog then lifts its head and begins licking the face of its friend, seemingly in celebration or affectionate greeting. This touching moment between the two dogs conveys a sense of companionship and mutual affection, and highlights the deep bond shared by these two animals. \n\nThis scene depicts a typical domestic setting, where a dog is enjoying some leisure time outside with its owner. The brown dog is shown as friendly and well-adjusted, happily interacting with both its human companion and another dog. The inclusion of various details such as the dog’s color, collar, and behavior convey a sense of realism and authenticity, enhancing the overall impact of the scene. Overall, this brief vignette presents a charming and heartwarming depiction of the special relationship between dogs and their owners, highlighting the deep bonds of friendship and loyalty which exist between these two species.\n\n","output_type":"stream"}]}]}